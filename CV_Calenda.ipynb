{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4NT4qzR52o+lFAwleM94m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ML-ctrl/Thesis-ML_2022/blob/main/CV_Calenda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the data from Google Drive."
      ],
      "metadata": {
        "id": "W-JNZ5xIuv2K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "P4ooR9yHPlHg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cb18b78-2c8e-45f9-8231-21970fc9094d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Oh2KZXteF4FyhDO7YXi3X0kOn-yOY_5z\n",
            "To: /content/Calenda_posts.xlsx\n",
            "100%|██████████| 34.5k/34.5k [00:00<00:00, 21.0MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             caption                   id  \\\n",
            "0                                            caption                   id   \n",
            "1  Bonomi ha ragione. È incredibile il fatto che ...  3322110098234972018   \n",
            "2  Parlare ad ogni elezione di “vento che cambia”...  3321294674173990657   \n",
            "3  Il Patto repubblicano. \\n\\n1) Stati Uniti d’Eu...  3260379198833313961   \n",
            "4  Abbiamo analizzato il “maestrale” della Sardeg...  3321984442625111754   \n",
            "\n",
            "                                        url  \n",
            "0                                       url  \n",
            "1  https://www.instagram.com/p/C4agYh2CJdy/  \n",
            "2  https://www.instagram.com/p/C4Xm-jAiZcB/  \n",
            "3  https://www.instagram.com/p/C0_MZtriEip/  \n",
            "4  https://www.instagram.com/p/C4aDz_8CzbK/  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import gdown\n",
        "\n",
        "# Download the file using gdown\n",
        "url = \"https://drive.google.com/uc?id=1Oh2KZXteF4FyhDO7YXi3X0kOn-yOY_5z\"\n",
        "output = 'Calenda_posts.xlsx'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Read the downloaded Excel file into a Pandas DataFrame\n",
        "df = pd.read_excel(output, header=None, names=['caption', 'id', 'url'])\n",
        "\n",
        "# Display the first few rows of the DataFrame to understand what it looks like\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display all images"
      ],
      "metadata": {
        "id": "U1v7n2vav2wi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "# Load the Excel file\n",
        "excel_file = \"Calenda_posts.xlsx\"\n",
        "df = pd.read_excel(excel_file)\n",
        "\n",
        "# Iterate through the URLs in the 'url' column\n",
        "for index, row in df.iterrows():\n",
        "    # Extract the Instagram post URL from the current row\n",
        "    instagram_url = row['url']\n",
        "\n",
        "    # Send a GET request to the Instagram post URL\n",
        "    response = requests.get(instagram_url)\n",
        "\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content of the page\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find the meta tag containing the image URL\n",
        "        meta_tag = soup.find('meta', property='og:image')\n",
        "        if meta_tag:\n",
        "            # Extract the content attribute (image URL) from the meta tag\n",
        "            image_url = meta_tag['content']\n",
        "\n",
        "            # Send a GET request to download the image\n",
        "            image_response = requests.get(image_url)\n",
        "\n",
        "            # Check if the image request was successful (status code 200)\n",
        "            if image_response.status_code == 200:\n",
        "                # Open the image using PIL (Python Imaging Library)\n",
        "                img = Image.open(BytesIO(image_response.content))\n",
        "\n",
        "                # Display the image\n",
        "                plt.imshow(img)\n",
        "                plt.axis('off')\n",
        "                plt.title(instagram_url)\n",
        "                plt.show()\n",
        "            else:\n",
        "                print(f\"Failed to download image from {instagram_url}\")\n",
        "        else:\n",
        "            print(f\"Image URL not found in {instagram_url}\")\n",
        "    else:\n",
        "        print(f\"Failed to fetch Instagram post from {instagram_url}\")"
      ],
      "metadata": {
        "id": "rupnGAVPUcNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download all images and save them in a new directory"
      ],
      "metadata": {
        "id": "WJKdZ6KWv48h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "# Create a directory for saving downloaded images\n",
        "download_dir = \"url_downloaded\"\n",
        "if not os.path.exists(download_dir):\n",
        "    os.makedirs(download_dir)\n",
        "\n",
        "# Load the Excel file\n",
        "excel_file = \"Calenda_posts.xlsx\"\n",
        "df = pd.read_excel(excel_file)\n",
        "\n",
        "# Iterate through the URLs in the 'url' column\n",
        "for index, row in df.iterrows():\n",
        "    # Extract the Instagram post URL from the current row\n",
        "    instagram_url = row['url']\n",
        "\n",
        "    # Send a GET request to the Instagram post URL\n",
        "    response = requests.get(instagram_url)\n",
        "\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content of the page\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find the meta tag containing the image URL\n",
        "        meta_tag = soup.find('meta', property='og:image')\n",
        "        if meta_tag:\n",
        "            # Extract the content attribute (image URL) from the meta tag\n",
        "            image_url = meta_tag['content']\n",
        "\n",
        "            # Send a GET request to download the image\n",
        "            image_response = requests.get(image_url)\n",
        "\n",
        "            # Check if the image request was successful (status code 200)\n",
        "            if image_response.status_code == 200:\n",
        "                # Open the image using PIL (Python Imaging Library)\n",
        "                img = Image.open(BytesIO(image_response.content))\n",
        "\n",
        "                # Save the image to the download directory\n",
        "                image_filename = f\"{index}.jpg\"\n",
        "                image_path = os.path.join(download_dir, image_filename)\n",
        "                img.save(image_path)\n",
        "\n",
        "                print(f\"Image downloaded and saved: {image_path}\")\n",
        "            else:\n",
        "                print(f\"Failed to download image from {instagram_url}\")\n",
        "        else:\n",
        "            print(f\"Image URL not found in {instagram_url}\")\n",
        "    else:\n",
        "        print(f\"Failed to fetch Instagram post from {instagram_url}\")"
      ],
      "metadata": {
        "id": "tWhx4hvBXHfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize the images"
      ],
      "metadata": {
        "id": "YSrPjLOhwAya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#IMAGE NORMALIZATION\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def resize_and_crop(img, size):\n",
        "    # Resize image to maintain aspect ratio\n",
        "    h, w, _ = img.shape\n",
        "    if h > w:\n",
        "        new_h, new_w = size * h / w, size\n",
        "    else:\n",
        "        new_h, new_w = size, size * w / h\n",
        "\n",
        "    new_h, new_w = int(new_h), int(new_w)\n",
        "    resized_img = cv2.resize(img, (new_w, new_h))\n",
        "\n",
        "    # Crop the center of the image\n",
        "    startx = new_w//2 - size//2\n",
        "    starty = new_h//2 - size//2\n",
        "    return resized_img[starty:starty+size, startx:startx+size]\n",
        "\n",
        "def normalize_image(img):\n",
        "    # Normalize pixel values to [0, 1]\n",
        "    return img / 255.0\n",
        "\n",
        "directory = 'url_downloaded'\n",
        "output_directory = 'images'\n",
        "\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
        "#    if filename.endswith('.jpg'):\n",
        "        img = cv2.imread(os.path.join(directory, filename))\n",
        "        img = resize_and_crop(img, 256)\n",
        "        normalized_img = normalize_image(img)\n",
        "\n",
        "        # Convert the normalized image back to 8-bit format\n",
        "        img_to_save = (normalized_img * 255).astype(np.uint8)\n",
        "\n",
        "        # Save the normalized image\n",
        "        output_path = os.path.join(output_directory, filename)\n",
        "        cv2.imwrite(output_path, img_to_save)"
      ],
      "metadata": {
        "id": "TWJovNBsYgxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate color histogram and histogram similarity matrix"
      ],
      "metadata": {
        "id": "2szyJ7CIwRBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def compute_histogram(image, bins=256):\n",
        "    \"\"\"Compute the color histogram for an image.\"\"\"\n",
        "    histogram = [cv2.calcHist([image], [i], None, [bins], [0, 256]) for i in range(3)]\n",
        "    return np.concatenate(histogram).flatten()\n",
        "\n",
        "def calculate_similarity(hist_list):\n",
        "    \"\"\"Calculate histogram similarity matrix.\"\"\"\n",
        "    num_images = len(hist_list)\n",
        "    similarity_matrix = np.zeros((num_images, num_images))\n",
        "\n",
        "    for i in range(num_images):\n",
        "        for j in range(num_images):\n",
        "            similarity = cv2.compareHist(hist_list[i], hist_list[j], cv2.HISTCMP_CORREL)\n",
        "            similarity_matrix[i, j] = similarity\n",
        "\n",
        "    return similarity_matrix\n",
        "\n",
        "# Directory containing images\n",
        "image_directory = 'images'\n",
        "\n",
        "# Load and process images\n",
        "image_files = [f for f in os.listdir(image_directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "histograms = []\n",
        "\n",
        "for file in image_files:\n",
        "    image_path = os.path.join(image_directory, file)\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is not None:\n",
        "        hist = compute_histogram(image)\n",
        "        histograms.append(hist)\n",
        "\n",
        "# Calculate the similarity matrix\n",
        "similarity_matrix = calculate_similarity(histograms)\n",
        "\n",
        "# Plotting the similarity matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(similarity_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title('Image Histogram Similarity Matrix')\n",
        "plt.xlabel('Image Index')\n",
        "plt.ylabel('Image Index')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ogs-Ut7nYmZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hierarchical clusterign dendogram"
      ],
      "metadata": {
        "id": "ZgyIyLXqw61n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Directory containing images\n",
        "image_directory = 'images'\n",
        "\n",
        "# Load and process images\n",
        "image_files = [f for f in os.listdir(image_directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "histograms = []\n",
        "\n",
        "for file in image_files:\n",
        "    image_path = os.path.join(image_directory, file)\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is not None:\n",
        "        hist = compute_histogram(image)\n",
        "        histograms.append(hist)\n",
        "\n",
        "# Calculate the similarity matrix\n",
        "similarity_matrix = calculate_similarity(histograms)\n",
        "\n",
        "# Perform hierarchical clustering\n",
        "Z = linkage(1 - similarity_matrix, method='average')\n",
        "\n",
        "# Plot dendrogram\n",
        "plt.figure(figsize=(12, 8))\n",
        "dendrogram(Z, labels=image_files, orientation='right')\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.xlabel('Distance')\n",
        "plt.ylabel('Image')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7ABFHtj-YuN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Directory containing images\n",
        "image_directory = 'images'\n",
        "\n",
        "# Load and process images\n",
        "image_files = [f for f in os.listdir(image_directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "histograms = []\n",
        "\n",
        "for file in image_files:\n",
        "    image_path = os.path.join(image_directory, file)\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is not None:\n",
        "        hist = compute_histogram(image)\n",
        "        histograms.append(hist)\n",
        "\n",
        "# Calculate the similarity matrix\n",
        "similarity_matrix = calculate_similarity(histograms)\n",
        "\n",
        "# Perform hierarchical clustering\n",
        "Z = linkage(1 - similarity_matrix, method='average')\n",
        "\n",
        "# Plot dendrogram\n",
        "plt.figure(figsize=(12, 8))\n",
        "dendrogram(Z, labels=image_files, orientation='right')\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.xlabel('Distance')\n",
        "plt.ylabel('Image')\n",
        "plt.tick_params(axis='y', which='both', left=False, labelleft=False)  # Suppress tick labels on y-axis\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lkX92-n_A9Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display examples of images from each cluster"
      ],
      "metadata": {
        "id": "_QJXBkdBw_6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
        "\n",
        "# Define function to pick random images from clusters\n",
        "def pick_random_images(image_files, clusters, num_images_per_cluster=3):\n",
        "    random_images = []\n",
        "    unique_clusters = np.unique(clusters)\n",
        "\n",
        "    for cluster_id in unique_clusters:\n",
        "        cluster_indices = np.where(clusters == cluster_id)[0]\n",
        "        if len(cluster_indices) >= num_images_per_cluster:\n",
        "            random_indices = np.random.choice(cluster_indices, num_images_per_cluster, replace=False)\n",
        "            random_images.extend([(image_files[i], cluster_id) for i in random_indices])\n",
        "        else:\n",
        "            random_images.extend([(image_files[i], cluster_id) for i in cluster_indices])\n",
        "\n",
        "    return random_images\n",
        "\n",
        "# Directory containing images\n",
        "image_directory = 'images'\n",
        "\n",
        "# Load and process images\n",
        "image_files = [f for f in os.listdir(image_directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "histograms = []\n",
        "\n",
        "for file in image_files:\n",
        "    image_path = os.path.join(image_directory, file)\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is not None:\n",
        "        hist = compute_histogram(image)\n",
        "        histograms.append(hist)\n",
        "\n",
        "# Calculate the similarity matrix\n",
        "similarity_matrix = calculate_similarity(histograms)\n",
        "\n",
        "# Perform hierarchical clustering\n",
        "Z = linkage(1 - similarity_matrix, method='average')\n",
        "\n",
        "# Determine clusters\n",
        "num_clusters = 2\n",
        "clusters = fcluster(Z, num_clusters, criterion='maxclust')\n",
        "\n",
        "# Pick random images from clusters\n",
        "random_images = pick_random_images(image_files, clusters)\n",
        "\n",
        "# Display random images grouped by cluster\n",
        "plt.figure(figsize=(15, 8))\n",
        "for i, (image_name, cluster_id) in enumerate(random_images, start=1):\n",
        "    image_path = os.path.join(image_directory, image_name)\n",
        "    image = cv2.imread(image_path)\n",
        "    plt.subplot(4, 3, i)\n",
        "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(f'Cluster {cluster_id}: {image_name}')\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "amKAgkTrZRbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBJECT DETECTION WITH COCO"
      ],
      "metadata": {
        "id": "wrcHbzYjpElI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install pyyaml==5.1\n",
        "import sys, os, distutils.core\n",
        "\n",
        "!git clone 'https://github.com/facebookresearch/detectron2'\n",
        "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "sys.path.insert(0, os.path.abspath('./detectron2'))"
      ],
      "metadata": {
        "id": "0sVnkJRcZWLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)"
      ],
      "metadata": {
        "id": "wxeKgnJBZa7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog"
      ],
      "metadata": {
        "id": "wUzTxanAZ6Lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "PE1sbGxiZ-Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "from collections import Counter\n",
        "\n",
        "# Image directory\n",
        "image_directory = \"images\"\n",
        "\n",
        "# Inference with COCO panoptic segmentation model\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\"))\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\")\n",
        "\n",
        "# Set to run on CPU\n",
        "cfg.MODEL.DEVICE = \"cpu\"\n",
        "# Initialize the predictor\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# Get COCO metadata to map category IDs to category names\n",
        "coco_metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])\n",
        "\n",
        "# List to store information for plotting\n",
        "all_object_counts = Counter()\n",
        "\n",
        "# Iterate over image files in the directory\n",
        "for image_file in os.listdir(image_directory):\n",
        "    if image_file.endswith(('.jpg', '.jpeg', '.png')):\n",
        "      # Read the image\n",
        "        img_path = os.path.join(image_directory, image_file)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # Make predictions\n",
        "        panoptic_seg, segments_info = predictor(img)[\"panoptic_seg\"]\n",
        "\n",
        "        # Count occurrences of all object categories\n",
        "        category_ids = [segment_info['category_id'] for segment_info in segments_info]\n",
        "        all_object_counts.update(category_ids)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_n6lVXfaCO0",
        "outputId": "b07a1255-39c9-421a-dfda-f3865480d7de"
      },
      "execution_count": 15,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[05/29 23:19:49 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-PanopticSegmentation/panoptic_fpn_R_101_3x/139514519/model_final_cafdb1.pkl ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Object frequency"
      ],
      "metadata": {
        "id": "FyUkY3aKxID3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# Bar plot for all object counts\n",
        "objects_names = [coco_metadata.thing_classes[category_id] for category_id in all_object_counts.keys()]\n",
        "plt.bar(objects_names, all_object_counts.values())\n",
        "plt.xlabel('COCO Object Name')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Object frequency')\n",
        "plt.xticks(rotation=90, ha='right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WuOzmeDVcDcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Object frequency with percentages"
      ],
      "metadata": {
        "id": "UWI3lTFGxOKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# Bar plot for all object counts\n",
        "objects_names = [coco_metadata.thing_classes[category_id] for category_id in all_object_counts.keys()]\n",
        "object_frequencies = list(all_object_counts.values())\n",
        "total_objects = sum(object_frequencies)\n",
        "\n",
        "# Calculate percentages\n",
        "percentages = [(freq / total_objects) * 100 for freq in object_frequencies]\n",
        "\n",
        "plt.bar(objects_names, percentages)\n",
        "plt.xlabel('COCO Object Name')\n",
        "plt.ylabel('Frequency (%)')\n",
        "plt.title('Object frequency (%)')\n",
        "plt.xticks(rotation=90, ha='right')\n",
        "\n",
        "# Add percentages on top of bars\n",
        "for i, percentage in enumerate(percentages):\n",
        "    plt.text(i, percentage + 0.5, f\"{percentage:.2f}%\", ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OWgPzZPZfRmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "List of object frequencies"
      ],
      "metadata": {
        "id": "YfHyFirsxXog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate percentages\n",
        "percentages = [(freq / total_objects) * 100 for freq in object_frequencies]\n",
        "\n",
        "# Display object names and their respective percentages\n",
        "for name, percentage in zip(objects_names, percentages):\n",
        "    print(f\"{name}: {percentage:.2f}%\")"
      ],
      "metadata": {
        "id": "Z8771WaZfSnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I was surprised by the high percentage of the category \"hot dog\", so I decided inspect the pictures that contained this object."
      ],
      "metadata": {
        "id": "vFuNVl1BxjQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Image directory\n",
        "image_directory = \"images\"\n",
        "\n",
        "# Inference with COCO panoptic segmentation model\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\"))\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\")\n",
        "\n",
        "# Set to run on CPU\n",
        "cfg.MODEL.DEVICE = \"cpu\"\n",
        "# Initialize the predictor\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# Get COCO metadata to map category IDs to category names\n",
        "coco_metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])\n",
        "\n",
        "# Iterate over image files in the directory\n",
        "for image_file in os.listdir(image_directory):\n",
        "    if image_file.endswith(('.jpg', '.jpeg', '.png')):\n",
        "        # Read the image\n",
        "        img_path = os.path.join(image_directory, image_file)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # Make predictions\n",
        "        panoptic_seg, segments_info = predictor(img)[\"panoptic_seg\"]\n",
        "\n",
        "        # Check if \"hot dog\" category is detected\n",
        "        contains_hot_dog = False\n",
        "        for segment_info in segments_info:\n",
        "            category_id = segment_info['category_id']\n",
        "            category_name = coco_metadata.thing_classes[category_id]\n",
        "            if category_name == \"hot dog\":\n",
        "                contains_hot_dog = True\n",
        "                break\n",
        "\n",
        "        # Display the image if it contains a \"hot dog\"\n",
        "        if contains_hot_dog:\n",
        "            plt.figure(figsize=(10, 10))\n",
        "            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "            plt.title(\"Image with hot dog: \" + image_file)\n",
        "            plt.axis('off')\n",
        "            plt.show()\n"
      ],
      "metadata": {
        "id": "adrIBI0ekm3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As seen in the above pictures, they don't actually contain hot dogs.\n",
        "This is understable if you consider that models trained exclusively on panoptic segmentation datasets like COCO may not generalize well to real-world scenarios, which could include different lighting conditions, object orientations, or occlusions. Moreover, one limitation is the inherent bias in the dataset due to class imbalance, which can affect the performance of the models. COCO's design inevitably encompassed certain annotation biases, including issues like imprecise object boundaries and incorrect class labels.\n",
        "Given that the pictures don't contain hot dogs, I decided to exclude this object from the frequency graph."
      ],
      "metadata": {
        "id": "amt9iPOZuOHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filter out \"hot dog\" category\n",
        "filtered_object_counts = {category_id: count for category_id, count in all_object_counts.items() if coco_metadata.thing_classes[category_id] != \"hot dog\"}\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# Bar plot for filtered object counts\n",
        "objects_names = [coco_metadata.thing_classes[category_id] for category_id in filtered_object_counts.keys()]\n",
        "plt.bar(objects_names, filtered_object_counts.values())\n",
        "plt.xlabel('COCO Object Name')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title (\"Object frequency\")\n",
        "plt.xticks(rotation=90, ha='right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sDuXnaRjlrPS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}